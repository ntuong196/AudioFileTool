{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73694c4",
   "metadata": {},
   "source": [
    "# Audiobook Generator (XTTS v2 model) — Notebook\n",
    "\n",
    "This notebook generates an audiobook-style audio file from a **TXT** file using a **reference voice sample** (**MP3**) for speaker conditioning via **Coqui TTS (XTTS v2)**.\n",
    "\n",
    "**Folder assumption:** the reference voice MP3 and the input TXT are in the **same folder** as this notebook (or you can point to a different folder).\n",
    "\n",
    "**Ethics/safety:** Only generate a voice you own or have **explicit permission** to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee09617",
   "metadata": {},
   "source": [
    "## 0. Install the dependencies: \n",
    "- `coqui-tts` for XTTS v2\n",
    "- `ffmpeg` is required for MP3 I/O (conversion + final MP3 export)\n",
    "\n",
    "If you don't have FFmpeg installed:\n",
    "- Windows: install via `choco install ffmpeg` (Chocolatey) or download an official build (\"ffmpeg-7.1.1-full_build-shared.7z\") and add to PATH\n",
    "- macOS: `brew install ffmpeg`\n",
    "- Linux (Debian/Ubuntu): `sudo apt-get install ffmpeg`\n",
    "\n",
    "Then run: `pip install -r requirements.txt`\n",
    "\n",
    "or like me (Windows):\n",
    "\n",
    "```bash\n",
    "python -m venv audiogen\n",
    "audiogen/Scripts/activate\n",
    "pip install ipykernel, coqui-tts\n",
    "pip install \"transformers==5.0.0\"\n",
    "uv pip install torch torchaudio torchcodec --torch-backend=auto\n",
    "git clone https://github.com/idiap/coqui-ai-TTS\n",
    "cd coqui-ai-TTS\n",
    "uv pip install -e .[notebooks]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49316e63",
   "metadata": {},
   "source": [
    "## 1. Imports and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, torch, shutil, subprocess\n",
    "from pydub import AudioSegment\n",
    "from TTS.api import TTS\n",
    "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
    "\n",
    "# # Get device if NVDIA GPU is present. AMD GPU is not supported or very limit.\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(TTS().list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "903370e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the location of cached voice\n",
    "WORKDIR = os.getcwd()\n",
    "if (\"XDG_DATA_HOME\" not in os.environ) and (\"TTS_HOME\" not in os.environ):\n",
    "\tos.environ[\"XDG_DATA_HOME\"] = WORKDIR + \"\\\\temp\"\n",
    "\tos.environ[\"TTS_HOME\"] = WORKDIR + \"\\\\temp\"\n",
    "\n",
    "# Set location for input files\n",
    "REF_VOICES = WORKDIR + \"\\\\voices\"\n",
    "INPUT_TEXTS = WORKDIR + \"\\\\texts\"\n",
    "\n",
    "# Set location for output files\n",
    "OUTPUT_CHUNKS = WORKDIR + \"\\\\chunks\"\n",
    "OUTPUT_AUDIOS = WORKDIR + \"\\\\audios\"\n",
    "\n",
    "# Pick the file for now\n",
    "VOICE_NAME = \"Hor_Tuck_Loon\"\n",
    "FILE_NAME = \"thought_out_thought\"\n",
    "\n",
    "# Output tuning\n",
    "SPEED = 1.0                 # 1.0 = normal, 1.1 = 10% faster, 0.9 = 10% slower\n",
    "MP3_BITRATE = \"192k\"        # e.g. \"96k\", \"128k\", \"192k\", \"256k\"\n",
    "\n",
    "NORMALIZE_LOUDNESS = True   # loudnorm\n",
    "LOUDNORM_I = -16            # Integrated loudness target (LUFS). Common: -16 (podcast/audiobook)\n",
    "LOUDNORM_TP = -1.5          # True peak (dBTP)\n",
    "LOUDNORM_LRA = 11           # Loudness range\n",
    "\n",
    "TRIM_SILENCE = False         # silenceremove\n",
    "SILENCE_THRESHOLD_DB = -45  # threshold in dB for silence detection\n",
    "SILENCE_MIN_SEC = 0.20      # min silence duration to trim at start/end (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8764292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "\t# Remove hyphenation at line breaks: \"exam-\\nple\" -> \"example\"\n",
    "\ttext = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n",
    "\t# Collapse whitespace\n",
    "\ttext = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\ttext = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\treturn text.strip()\n",
    "\n",
    "def mp3_to_wav(mp3_path, wav_path):\n",
    "    \"\"\"Convert MP3 -> WAV using pydub (requires ffmpeg).\"\"\"\n",
    "    audio = AudioSegment.from_file(mp3_path, format=\"mp3\")\n",
    "    audio = audio.set_channels(1)  # mono often works better for speaker conditioning\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93808cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _atempo_chain(speed: float) -> str:\n",
    "    \"\"\"Build chained atempo filters to support speeds outside 0.5..2.0.\"\"\"\n",
    "    if speed <= 0:\n",
    "        raise ValueError(\"SPEED must be > 0\")\n",
    "\n",
    "    parts = []\n",
    "    x = float(speed)\n",
    "\n",
    "    while x > 2.0:\n",
    "        parts.append(\"atempo=2.0\")\n",
    "        x /= 2.0\n",
    "    while x < 0.5:\n",
    "        parts.append(\"atempo=0.5\")\n",
    "        x /= 0.5\n",
    "\n",
    "    parts.append(f\"atempo={x:.6f}\".rstrip(\"0\").rstrip(\".\"))\n",
    "    return \",\".join(parts)\n",
    "\n",
    "def _silenceremove_filter(threshold_db: float, min_sec: float) -> str:\n",
    "    \"\"\"Trim leading+trailing silence using silenceremove.\"\"\"\n",
    "    thr = f\"{threshold_db}dB\"\n",
    "    d = max(0.0, float(min_sec))\n",
    "    return (\n",
    "        f\"silenceremove=\"\n",
    "        f\"start_periods=1:start_duration={d}:start_threshold={thr}:\"\n",
    "        f\"stop_periods=1:stop_duration={d}:stop_threshold={thr}\"\n",
    "    )\n",
    "\n",
    "def _loudnorm_filter(I: float, TP: float, LRA: float) -> str:\n",
    "    \"\"\"EBU R128 loudness normalization (single-pass).\"\"\"\n",
    "    return f\"loudnorm=I={I}:TP={TP}:LRA={LRA}\"\n",
    "\n",
    "def export_with_ffmpeg_filters(\n",
    "    audio: AudioSegment,\n",
    "    out_path: Path,\n",
    "    speed: float = 1.0,\n",
    "    mp3_bitrate: str = \"192k\",\n",
    "    trim_silence: bool = True,\n",
    "    silence_threshold_db: float = -45,\n",
    "    silence_min_sec: float = 0.20,\n",
    "    normalize_loudness: bool = True,\n",
    "    loudnorm_I: float = -16,\n",
    "    loudnorm_TP: float = -1.5,\n",
    "    loudnorm_LRA: float = 11,\n",
    "):\n",
    "    \"\"\"Export AudioSegment applying FFmpeg filters: silenceremove, atempo, loudnorm, and bitrate.\"\"\"\n",
    "    if shutil.which(\"ffmpeg\") is None:\n",
    "        raise RuntimeError(\"ffmpeg not found on PATH. Install ffmpeg to use MP3 I/O and audio filtering.\")\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_ext = out_path.suffix.lower()\n",
    "\n",
    "    # Write a temp WAV from pydub, then let FFmpeg filter+encode.\n",
    "    tmp_wav = out_path.with_suffix(\".tmp_export.wav\")\n",
    "    audio.export(tmp_wav, format=\"wav\")\n",
    "\n",
    "    filters = []\n",
    "    if trim_silence:\n",
    "        filters.append(_silenceremove_filter(silence_threshold_db, silence_min_sec))\n",
    "    if abs(speed - 1.0) > 1e-6:\n",
    "        filters.append(_atempo_chain(speed))\n",
    "    if normalize_loudness:\n",
    "        filters.append(_loudnorm_filter(loudnorm_I, loudnorm_TP, loudnorm_LRA))\n",
    "\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", str(tmp_wav)]\n",
    "\n",
    "    if filters:\n",
    "        cmd += [\"-filter:a\", \",\".join(filters)]\n",
    "\n",
    "    if out_ext == \".mp3\":\n",
    "        cmd += [\"-b:a\", mp3_bitrate, str(out_path)]\n",
    "    elif out_ext == \".wav\":\n",
    "        cmd += [\"-c:a\", \"pcm_s16le\", str(out_path)]\n",
    "    else:\n",
    "        raise ValueError(\"OUT_AUDIO must end with .wav or .mp3\")\n",
    "\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    try:\n",
    "        tmp_wav.unlink()\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = f\"{INPUT_TEXTS}\\\\{FILE_NAME}.txt\"\n",
    "contents = []\n",
    "with open(file_path, 'r') as file:\n",
    "\tfor line in file:\n",
    "\t\tif line.strip():\n",
    "\t\t\tcontents.append(normalize_text(line))\n",
    "\n",
    "# i=1\n",
    "# for para in contents:\n",
    "# \tprint(f\"{i}. {para}\")\n",
    "# \ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d03731",
   "metadata": {},
   "source": [
    "## 2. Provide reference audio files and generate a sample simple speech with that voice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22727b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d9bee9c9c84a4db0091122f3a80184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a208530559b48219d304cc77d778766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize TTS model\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfed889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.wav'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample speech\n",
    "# reference_files = [\"reference_voice.mp3\"]\n",
    "# tts.tts_to_file(\n",
    "#     text=\"This is the audio edtion of the book . \\\"Thought . Our Sole Universe\\\"\",\n",
    "#     speaker_wav=reference_files,\n",
    "#     language=\"en\",\n",
    "#     file_path=\"output.wav\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47fa5d",
   "metadata": {},
   "source": [
    "## 3. Cache reference voices for easy reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "885d0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference audio\n",
    "reference_files = f\"{REF_VOICES}\\\\{VOICE_NAME}.mp3\"\n",
    "\n",
    "chunk_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of temporary chunks files\n",
    "file_name = f\"{OUTPUT_CHUNKS}\\\\{FILE_NAME}\\\\output0.wav\"\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "\ttts.tts_to_file(\n",
    "\t\ttext=\"This is the audio edition of the book - “Thought - Our.. Sole Universe”\",\n",
    "\t\tspeaker_wav=reference_files,\n",
    "\t\tspeaker=\"HorTuckLoon\",\n",
    "\t\tlanguage=\"en\",\n",
    "\t\tfile_path=file_name\n",
    "\t)\n",
    "\tchunk_files.append(file_name)\n",
    "\n",
    "# print(chunk_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99deae6",
   "metadata": {},
   "source": [
    "Create a temporary folder like `thought_out_thought/` with many chunk output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for paragraph in contents:\n",
    "\tfile_name = f\"{OUTPUT_CHUNKS}\\\\{FILE_NAME}\\\\output{i}.wav\"\n",
    "\tif not os.path.exists(file_name):\n",
    "\t\ttts.tts_to_file(\n",
    "\t\t\ttext=paragraph,\n",
    "\t\t\tspeaker=\"HorTuckLoon\",\n",
    "\t\t\tlanguage=\"en\",\n",
    "\t\t\tfile_path=file_name\n",
    "\t\t)\n",
    "\t\tchunk_files.append(file_name)\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "378be76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output0.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output1.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output2.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output3.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output4.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output5.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output6.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output7.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output8.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output9.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output10.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output11.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output12.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output13.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output14.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output15.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output16.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output17.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output18.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output19.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output20.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output21.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output22.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output23.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output24.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output25.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output26.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output27.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output28.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output29.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output30.wav', 'd:\\\\Dhamma\\\\AudioFileTool\\\\chunks\\\\thought_out_thought\\\\output31.wav']\n"
     ]
    }
   ],
   "source": [
    "def natural_sort_key(s):\n",
    "    # Split the filename into parts of numbers and non-numbers\n",
    "    return [int(p) if p.isdigit() else p.lower() for p in re.findall(r'\\d+|\\D+', s)]\n",
    "\n",
    "chunk_files.sort(key=natural_sort_key)\n",
    "print(chunk_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713a226",
   "metadata": {},
   "source": [
    "## 4. Concatenate into a single audio file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b433e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\thought_out_thought.mp3!\n"
     ]
    }
   ],
   "source": [
    "# Concatenate chunk WAVs into one AudioSegment\n",
    "combined = AudioSegment.empty()\n",
    "audio_file = f\"{OUTPUT_AUDIOS}\\\\{FILE_NAME}.mp3\"\n",
    "\n",
    "for file in chunk_files:\n",
    "    seg = AudioSegment.from_wav(file)\n",
    "    combined += seg\n",
    "\n",
    "try:\n",
    "    combined.export(audio_file, format=\"mp3\", bitrate=\"128k\")\n",
    "except:\n",
    "    print(f\"Can not combine chunk files!\")\n",
    "\n",
    "print(f\"Generated {audio_file}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
