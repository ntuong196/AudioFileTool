{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73694c4",
   "metadata": {},
   "source": [
    "# Audiobook Generator (XTTS v2 model) â€” Notebook\n",
    "\n",
    "This notebook generates an audiobook-style audio file from a **TXT** file using a **reference voice sample** (**MP3**) for speaker conditioning via **Coqui TTS (XTTS v2)**.\n",
    "\n",
    "**Folder assumption:** the reference voice MP3 and the input TXT are in the **same folder** as this notebook (or you can point to a different folder).\n",
    "\n",
    "**Ethics/safety:** Only generate a voice you own or have **explicit permission** to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee09617",
   "metadata": {},
   "source": [
    "## 0. Install the dependencies: \n",
    "- `coqui-tts` for XTTS v2\n",
    "- `ffmpeg` is required for MP3 I/O (conversion + final MP3 export)\n",
    "\n",
    "If you don't have FFmpeg installed:\n",
    "- Windows: install via `choco install ffmpeg` (Chocolatey) or download an official build (\"ffmpeg-7.1.1-full_build-shared.7z\") and add to PATH\n",
    "- macOS: `brew install ffmpeg`\n",
    "- Linux (Debian/Ubuntu): `sudo apt-get install ffmpeg`\n",
    "\n",
    "Then run: `pip install -r requirements.txt`\n",
    "\n",
    "or like me (Windows):\n",
    "\n",
    "```bash\n",
    "python -m venv audiogen\n",
    "audiogen/Scripts/activate\n",
    "pip install ipykernel, coqui-tts\n",
    "pip install \"transformers==5.0.0\"\n",
    "uv pip install torch torchaudio torchcodec --torch-backend=auto\n",
    "git clone https://github.com/idiap/coqui-ai-TTS\n",
    "cd coqui-ai-TTS\n",
    "uv pip install -e .[notebooks]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49316e63",
   "metadata": {},
   "source": [
    "## 1. Imports and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e60be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import torch\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from TTS.api import TTS\n",
    "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
    "\n",
    "# # Get device if NVDIA GPU is present. AMD GPU is not supported or very limit.\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(TTS().list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eef8d8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeed research more about this\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Need research more about this\n",
    "\"\"\"\n",
    "# WAV_FILE = filename = librosa.example('vibeace')\n",
    "# from TTS.config import BaseAudioConfig\n",
    "# from TTS.utils.audio import AudioProcessor\n",
    "# conf = BaseAudioConfig(pitch_fmax=640, pitch_fmin=1)\n",
    "# ap = AudioProcessor(**conf)\n",
    "# wav = ap.load_wav(WAV_FILE, sr=ap.sample_rate)[:5 * ap.sample_rate]\n",
    "# pitch = ap.compute_f0(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903370e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the location of cached voice\n",
    "WORKDIR = os.getcwd()\n",
    "if (\"XDG_DATA_HOME\" not in os.environ) and (\"TTS_HOME\" not in os.environ):\n",
    "\tos.environ[\"XDG_DATA_HOME\"] = WORKDIR + \"\\\\temp\"\n",
    "\tos.environ[\"TTS_HOME\"] = WORKDIR + \"\\\\temp\"\n",
    "\n",
    "# Set location for input files\n",
    "REF_VOICES = WORKDIR + \"\\\\voices\"\n",
    "INPUT_TEXTS = WORKDIR + \"\\\\texts\"\n",
    "\n",
    "# Set location for output files\n",
    "OUTPUT_CHUNKS = WORKDIR + \"\\\\chunks\"\n",
    "OUTPUT_AUDIOS = WORKDIR + \"\\\\audios\"\n",
    "\n",
    "# Pick the file for now\n",
    "# Should loop through all the file later\n",
    "# VOICE_NAME = \"HorTuckLoon_Talk\"\n",
    "# VOICE_NAME = \"HorTuckLoon_GP\"\n",
    "\n",
    "FILE_NAME = \"preface\"\n",
    "# FILE_NAME = \"thought_out_thought\"\n",
    "# FILE_NAME = \"way_of_thought\"\n",
    "# FILE_NAME = \"weight_of_thought\"\n",
    "\n",
    "\n",
    "# Output tuning\n",
    "SPEED = 0.9                 # 1.0 = normal, 1.1 = 10% faster, 0.9 = 10% slower\n",
    "MP3_BITRATE = \"192k\"        # e.g. \"96k\", \"128k\", \"192k\", \"256k\"\n",
    "\n",
    "NORMALIZE_LOUDNESS = True   # loudnorm\n",
    "LOUDNORM_I = -16            # Integrated loudness target (LUFS). Common: -16 (podcast/audiobook)\n",
    "LOUDNORM_TP = -1.5          # True peak (dBTP)\n",
    "LOUDNORM_LRA = 11           # Loudness range\n",
    "\n",
    "TRIM_SILENCE = False         # silenceremove\n",
    "SILENCE_THRESHOLD_DB = -45  # threshold in dB for silence detection\n",
    "SILENCE_MIN_SEC = 0.20      # min silence duration to trim at start/end (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8764292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise text file\n",
    "# Could write another one for different purposes\n",
    "# def normalize_text(text: str) -> str:\n",
    "# \t# Remove hyphenation at line breaks: \"exam-\\nple\" -> \"example\"\n",
    "# \ttext = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n",
    "# \t# Collapse whitespace\n",
    "# \ttext = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "# \ttext = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "# \treturn text.strip()\n",
    "\n",
    "def load_paragraphs(txt_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Remove line breaks inside paragraphs so each paragraph becomes one line.\n",
    "    2) Return a list of paragraphs: contents = [para1, para2, ...]\n",
    "\n",
    "    Paragraphs are separated by one or more blank lines.\n",
    "    \"\"\"\n",
    "    text = Path(txt_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    paragraphs = []\n",
    "    # Split on blank lines (handles Windows/Mac/Linux newlines)\n",
    "    # However can not group into one line\n",
    "    # for block in text.split('\\n\\n'):\n",
    "    #     paragraphs.append(block)\n",
    "\n",
    "    # A robust approach: split by blank lines using regex\n",
    "    import re\n",
    "    blocks = re.split(r\"(?:\\r?\\n){2,}\", text.strip())\n",
    "\n",
    "    for b in blocks:\n",
    "        # Collapse internal newlines/whitespace into single spaces\n",
    "        one_line = re.sub(r\"\\s*\\r?\\n\\s*\", \" \", b.strip())\n",
    "        # Collapse multiple spaces/tabs\n",
    "        one_line = re.sub(r\"[ \\t]{2,}\", \" \", one_line)\n",
    "        if one_line:\n",
    "            paragraphs.append(one_line)\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690d0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare method for wav conversion as wav is lossless, no compression, better data for training\n",
    "def mp3_to_wav(mp3_path, wav_path):\n",
    "    \"\"\"Convert MP3 -> WAV using pydub (requires ffmpeg).\"\"\"\n",
    "    audio = AudioSegment.from_file(mp3_path, format=\"mp3\")\n",
    "    audio = audio.set_channels(1)  # mono often works better for speaker conditioning\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93808cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio processing functions\n",
    "def _atempo_chain(speed: float) -> str:\n",
    "    \"\"\"Build chained atempo filters to support speeds outside 0.5..2.0.\"\"\"\n",
    "    if speed <= 0:\n",
    "        raise ValueError(\"SPEED must be > 0\")\n",
    "\n",
    "    parts = []\n",
    "    x = float(speed)\n",
    "\n",
    "    while x > 2.0:\n",
    "        parts.append(\"atempo=2.0\")\n",
    "        x /= 2.0\n",
    "    while x < 0.5:\n",
    "        parts.append(\"atempo=0.5\")\n",
    "        x /= 0.5\n",
    "\n",
    "    parts.append(f\"atempo={x:.6f}\".rstrip(\"0\").rstrip(\".\"))\n",
    "    return \",\".join(parts)\n",
    "\n",
    "def _silenceremove_filter(threshold_db: float, min_sec: float) -> str:\n",
    "    \"\"\"Trim leading+trailing silence using silenceremove.\"\"\"\n",
    "    thr = f\"{threshold_db}dB\"\n",
    "    d = max(0.0, float(min_sec))\n",
    "    return (\n",
    "        f\"silenceremove=\"\n",
    "        f\"start_periods=1:start_duration={d}:start_threshold={thr}:\"\n",
    "        f\"stop_periods=1:stop_duration={d}:stop_threshold={thr}\"\n",
    "    )\n",
    "\n",
    "def _loudnorm_filter(I: float, TP: float, LRA: float) -> str:\n",
    "    \"\"\"EBU R128 loudness normalization (single-pass).\"\"\"\n",
    "    return f\"loudnorm=I={I}:TP={TP}:LRA={LRA}\"\n",
    "\n",
    "def export_with_ffmpeg_filters(\n",
    "    audio: AudioSegment,\n",
    "    out_path: Path,\n",
    "    speed: float = 1.0,\n",
    "    mp3_bitrate: str = \"192k\",\n",
    "    trim_silence: bool = True,\n",
    "    silence_threshold_db: float = -45,\n",
    "    silence_min_sec: float = 0.20,\n",
    "    normalize_loudness: bool = True,\n",
    "    loudnorm_I: float = -16,\n",
    "    loudnorm_TP: float = -1.5,\n",
    "    loudnorm_LRA: float = 11,\n",
    "):\n",
    "    \"\"\"Export AudioSegment applying FFmpeg filters: silenceremove, atempo, loudnorm, and bitrate.\"\"\"\n",
    "    if shutil.which(\"ffmpeg\") is None:\n",
    "        raise RuntimeError(\"ffmpeg not found on PATH. Install ffmpeg to use MP3 I/O and audio filtering.\")\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_ext = out_path.suffix.lower()\n",
    "\n",
    "    # Write a temp WAV from pydub, then let FFmpeg filter+encode.\n",
    "    tmp_wav = out_path.with_suffix(\".tmp_export.wav\")\n",
    "    audio.export(tmp_wav, format=\"wav\")\n",
    "\n",
    "    filters = []\n",
    "    if trim_silence:\n",
    "        filters.append(_silenceremove_filter(silence_threshold_db, silence_min_sec))\n",
    "    if abs(speed - 1.0) > 1e-6:\n",
    "        filters.append(_atempo_chain(speed))\n",
    "    if normalize_loudness:\n",
    "        filters.append(_loudnorm_filter(loudnorm_I, loudnorm_TP, loudnorm_LRA))\n",
    "\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", str(tmp_wav)]\n",
    "\n",
    "    if filters:\n",
    "        cmd += [\"-filter:a\", \",\".join(filters)]\n",
    "\n",
    "    if out_ext == \".mp3\":\n",
    "        cmd += [\"-b:a\", mp3_bitrate, str(out_path)]\n",
    "    elif out_ext == \".wav\":\n",
    "        cmd += [\"-c:a\", \"pcm_s16le\", str(out_path)]\n",
    "    else:\n",
    "        raise ValueError(\"OUT_AUDIO must end with .wav or .mp3\")\n",
    "\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    try:\n",
    "        tmp_wav.unlink()\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea1061",
   "metadata": {},
   "source": [
    "## 2. Extract text from provided text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d36c8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old method to extract text files\n",
    "\n",
    "# file_path = f\"{INPUT_TEXTS}\\\\{FILE_NAME}.txt\"\n",
    "# contents = []\n",
    "# with open(file_path, 'r') as file:\n",
    "# \tfor line in file:\n",
    "# \t\tif line.strip():\n",
    "# \t\t\tcontents.append(normalize_text(line))\n",
    "\n",
    "# i=1\n",
    "# for para in contents:\n",
    "# \tprint(f\"{i}. {para}\")\n",
    "# \ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ec917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse ONE text file into `contents` object\n",
    "# file_path = f\"{INPUT_TEXTS}\\\\{FILE_NAME}.txt\"\n",
    "# contents = load_paragraphs(file_path)\n",
    "\n",
    "# # Print out chunk texts for monitoring text quality before TTS\n",
    "# i=1\n",
    "# for para in contents:\n",
    "# \tprint(f\"{i}. {para}\")\n",
    "# \ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08e1aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all text files into `contents` object\n",
    "contents: dict[str, list[str]] = {}\n",
    "\n",
    "for file in Path(INPUT_TEXTS).iterdir():\n",
    "\tfilename = file.name.split(\".\")[0]\n",
    "\tif file.suffix == \".txt\":\n",
    "\t\tfile_path = f\"{INPUT_TEXTS}\\\\{file.name}\"\n",
    "\t\tfile_chunks = load_paragraphs(file_path)\n",
    "\t\tcontents[filename] = file_chunks\n",
    "\n",
    "# # Print out chunk texts for monitoring text quality before TTS\n",
    "\n",
    "# for filename, content in contents.items():\n",
    "# \tprint(filename)\n",
    "# \ti=1\n",
    "# \tfor para in content:\n",
    "# \t\tprint(f\"{i}. {para}\")\n",
    "# \t\ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d03731",
   "metadata": {},
   "source": [
    "## 2. Load xTTSv2 model into `tts` intance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22727b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TTS model, download xtts_v2 model to local\n",
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
    "\n",
    "# After download, run the model locally\n",
    "tts = TTS(model_path=\"models\\\\tts_models--multilingual--multi-dataset--xtts_v2\\\\model.pth\",\n",
    "\t\t  config_path=\"models\\\\tts_models--multilingual--multi-dataset--xtts_v2\\\\config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66afb3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tts_paths(tts: \"TTS\") -> dict:\n",
    "#     '''\n",
    "# \tSupport function to check location of model\n",
    "#     '''\n",
    "#     if tts.synthesizer is None:\n",
    "#         raise RuntimeError(\"No TTS synthesizer loaded (did you load a VC model instead?)\")\n",
    "\n",
    "#     ckpt = Path(tts.synthesizer.tts_checkpoint)\n",
    "#     cfg  = Path(tts.synthesizer.tts_config_path)\n",
    "\n",
    "#     # If checkpoint is a file, model_root is its parent; if it's a dir, it is the dir itself.\n",
    "#     model_root = ckpt if ckpt.is_dir() else ckpt.parent\n",
    "\n",
    "#     return {\n",
    "#         \"model_name\": tts.model_name,\n",
    "#         \"checkpoint_path\": str(ckpt),\n",
    "#         \"config_path\": str(cfg),\n",
    "#         \"model_root\": str(model_root),\n",
    "#         \"voice_dir\": str(tts.synthesizer.voice_dir),\n",
    "#         \"download_base_default\": str(tts.manager.output_prefix),  # base folder for non-HF models\n",
    "#     }\n",
    "\n",
    "# paths = get_tts_paths(tts)\n",
    "# paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47fa5d",
   "metadata": {},
   "source": [
    "## 3. Cache reference voices for easy reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "885d0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the voice from reference audios - Dont have to run everytime\n",
    "# reference_files = []\n",
    "# for file in Path(REF_VOICES).iterdir():\n",
    "# \treference_files.append(f\"{REF_VOICES}\\\\{file.name}\")\n",
    "# output_voice = f\"{OUTPUT_CHUNKS}\\\\test_voice.wav\"\n",
    "\n",
    "# if reference_files:\n",
    "# \ttts.tts_to_file(\n",
    "# \t\ttext=\"This is the audio edition of the book - Thought - Our Sole Universe\",\n",
    "# \t\tspeaker_wav=reference_files,\n",
    "# \t\t# Assign `speakerID` to reuse later\n",
    "# \t\tspeaker=\"HorTuckLoon\",\n",
    "# \t\tlanguage=\"en\",\n",
    "# \t\tfile_path=output_voice\n",
    "# \t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65234b94",
   "metadata": {},
   "source": [
    "## 4. Generate audio chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99deae6",
   "metadata": {},
   "source": [
    "Create a temporary folder like `thought_out_thought/` with many chunk output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either reset or leave untouch the chunk_files object on a new run\n",
    "# chunk_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for paragraph in contents:\n",
    "# \t# names of temporary chunks files\n",
    "# \tfile_name = f\"{OUTPUT_CHUNKS}\\\\{FILE_NAME}\\\\output{i}.wav\"\n",
    "# \tif not os.path.exists(file_name):\n",
    "# \t\ttts.tts_to_file(\n",
    "# \t\t\ttext=paragraph,\n",
    "# \t\t\tspeaker=\"HorTuckLoon\",\n",
    "# \t\t\tlanguage=\"en\",\n",
    "# \t\t\tfile_path=file_name\n",
    "# \t\t)\n",
    "# \tif file_name not in chunk_files:\n",
    "# \t\tchunk_files.append(file_name)\n",
    "# \ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378be76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort the list to make sure the chunks is in the right orders\n",
    "# def natural_sort_key(s):\n",
    "#     # Split the filename into parts of numbers and non-numbers\n",
    "#     return [int(p) if p.isdigit() else p.lower() for p in re.findall(r'\\d+|\\D+', s)]\n",
    "\n",
    "# chunk_files.sort(key=natural_sort_key)\n",
    "# if len(chunk_files) != len(contents):\n",
    "#     raise ValueError(\"Error while creating chunk files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71dbed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either reset or leave untouch the chunk_files object on a new run\n",
    "chunk_files: dict[str, list[str]] = {}\n",
    "\n",
    "for filename, content in contents.items():\n",
    "\tchunk_files[filename] = []\n",
    "\ti=1\n",
    "\tfor paragraph in content:\n",
    "\t\tfilepath = f\"{OUTPUT_CHUNKS}\\\\{filename}\\\\output{i}.wav\"\n",
    "\t\tif not os.path.exists(filepath):\n",
    "\t\t\ttts.tts_to_file(\n",
    "\t\t\t\ttext=paragraph,\n",
    "\t\t\t\tspeaker=\"HorTuckLoon\",\n",
    "\t\t\t\tlanguage=\"en\",\n",
    "\t\t\t\tfile_path=filepath\n",
    "\t\t\t)\n",
    "\t\tif filepath not in chunk_files[filename]:\n",
    "\t\t\tchunk_files[filename].append(filepath)\n",
    "\t\ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b34bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the list to make sure the chunks is in the right orders\n",
    "def natural_sort_key(s):\n",
    "    # Split the filename into parts of numbers and non-numbers\n",
    "    return [int(p) if p.isdigit() else p.lower() for p in re.findall(r'\\d+|\\D+', s)]\n",
    "\n",
    "for filename, chunks in chunk_files.items():\n",
    "\tchunks.sort(key=natural_sort_key)\n",
    "\tif len(chunks) != len(contents[filename]):\n",
    "\t\traise ValueError(\"Error while creating chunk files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713a226",
   "metadata": {},
   "source": [
    "## 5. Concatenate into a single audio file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\preface.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\preface.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\preface.mp3!\n"
     ]
    }
   ],
   "source": [
    "# # Concatenate chunk WAVs into one AudioSegment\n",
    "# combined = AudioSegment.empty()\n",
    "# audio_file = f\"{OUTPUT_AUDIOS}\\\\{FILE_NAME}.mp3\"\n",
    "\n",
    "# for file in chunk_files:\n",
    "#     seg = AudioSegment.from_wav(file)\n",
    "#     combined += seg\n",
    "\n",
    "# try:\n",
    "# \texport_with_ffmpeg_filters(\n",
    "# \t\taudio=combined,\n",
    "# \t\tout_path=audio_file,\n",
    "# \t\tspeed=SPEED,\n",
    "# \t\tmp3_bitrate=MP3_BITRATE,\n",
    "# \t\ttrim_silence=TRIM_SILENCE,\n",
    "# \t\tsilence_threshold_db=SILENCE_THRESHOLD_DB,\n",
    "# \t\tsilence_min_sec=SILENCE_MIN_SEC,\n",
    "# \t\tnormalize_loudness=NORMALIZE_LOUDNESS,\n",
    "# \t\tloudnorm_I=LOUDNORM_I,\n",
    "# \t\tloudnorm_TP=LOUDNORM_TP,\n",
    "# \t\tloudnorm_LRA=LOUDNORM_LRA,\n",
    "# \t)\n",
    "# \tprint(f\"Generated {audio_file}!\")\n",
    "# except:\n",
    "#     print(f\"Error couldn't combine chunk files!\")\n",
    "# try:\n",
    "#     combined.export(audio_file, format=\"mp3\", bitrate=\"192k\")\n",
    "# except:\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923bca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\attention_and_thought.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\attention_and_thought.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\attention_and_thought.mp3!\n",
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\preface.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\preface.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\preface.mp3!\n",
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\thought_out_thought.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\thought_out_thought.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\thought_out_thought.mp3!\n",
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\way_of_thought.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\way_of_thought.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\way_of_thought.mp3!\n",
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\weight_of_thought.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\weight_of_thought.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\weight_of_thought.mp3!\n"
     ]
    }
   ],
   "source": [
    "for filename, chunks in chunk_files.items():\n",
    "\t# Concatenate chunk WAVs into one AudioSegment\n",
    "\tcombined = AudioSegment.empty()\n",
    "\taudio_file = f\"{OUTPUT_AUDIOS}\\\\{filename}.mp3\"\n",
    "\n",
    "\tfor file in chunks:\n",
    "\t\tseg = AudioSegment.from_wav(file)\n",
    "\t\tcombined += seg\n",
    "\n",
    "\ttry:\n",
    "\t\texport_with_ffmpeg_filters(\n",
    "\t\t\taudio=combined,\n",
    "\t\t\tout_path=audio_file,\n",
    "\t\t\tspeed=SPEED,\n",
    "\t\t\tmp3_bitrate=MP3_BITRATE,\n",
    "\t\t\ttrim_silence=TRIM_SILENCE,\n",
    "\t\t\tsilence_threshold_db=SILENCE_THRESHOLD_DB,\n",
    "\t\t\tsilence_min_sec=SILENCE_MIN_SEC,\n",
    "\t\t\tnormalize_loudness=NORMALIZE_LOUDNESS,\n",
    "\t\t\tloudnorm_I=LOUDNORM_I,\n",
    "\t\t\tloudnorm_TP=LOUDNORM_TP,\n",
    "\t\t\tloudnorm_LRA=LOUDNORM_LRA,\n",
    "\t\t)\n",
    "\t\tprint(f\"Generated {audio_file}!\")\n",
    "\texcept:\n",
    "\t\tprint(f\"Error couldn't combine chunk files of {filename}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
