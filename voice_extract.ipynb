{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73694c4",
   "metadata": {},
   "source": [
    "# Audiobook Generator (XTTS v2 model) — Notebook\n",
    "\n",
    "This notebook generates an audiobook-style audio file from a **TXT** file using a **reference voice sample** (**MP3**) for speaker conditioning via **Coqui TTS (XTTS v2)**.\n",
    "\n",
    "**Folder assumption:** the reference voice MP3 and the input TXT are in the **same folder** as this notebook (or you can point to a different folder).\n",
    "\n",
    "**Ethics/safety:** Only generate a voice you own or have **explicit permission** to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee09617",
   "metadata": {},
   "source": [
    "## 0. Install the dependencies: \n",
    "- `coqui-tts` for XTTS v2\n",
    "- `ffmpeg` is required for MP3 I/O (conversion + final MP3 export)\n",
    "\n",
    "If you don't have FFmpeg installed:\n",
    "- Windows: install via `choco install ffmpeg` (Chocolatey) or download an official build (\"ffmpeg-7.1.1-full_build-shared.7z\") and add to PATH\n",
    "- macOS: `brew install ffmpeg`\n",
    "- Linux (Debian/Ubuntu): `sudo apt-get install ffmpeg`\n",
    "\n",
    "Then run: `pip install -r requirements.txt`\n",
    "\n",
    "or like me (Windows):\n",
    "\n",
    "```bash\n",
    "python -m venv audiogen\n",
    "audiogen/Scripts/activate\n",
    "pip install ipykernel, coqui-tts\n",
    "pip install \"transformers==5.0.0\"\n",
    "uv pip install torch torchaudio torchcodec --torch-backend=auto\n",
    "git clone https://github.com/idiap/coqui-ai-TTS\n",
    "cd coqui-ai-TTS\n",
    "uv pip install -e .[notebooks]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49316e63",
   "metadata": {},
   "source": [
    "## 1. Imports and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e60be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# import torch\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from TTS.api import TTS\n",
    "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
    "\n",
    "# # Get device if NVDIA GPU is present. AMD GPU is not supported or very limit.\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(TTS().list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef8d8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeed research more about this\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Need research more about this\n",
    "\"\"\"\n",
    "# WAV_FILE = filename = librosa.example('vibeace')\n",
    "# from TTS.config import BaseAudioConfig\n",
    "# from TTS.utils.audio import AudioProcessor\n",
    "# conf = BaseAudioConfig(pitch_fmax=640, pitch_fmin=1)\n",
    "# ap = AudioProcessor(**conf)\n",
    "# wav = ap.load_wav(WAV_FILE, sr=ap.sample_rate)[:5 * ap.sample_rate]\n",
    "# pitch = ap.compute_f0(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903370e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the location of cached voice\n",
    "WORKDIR = os.getcwd()\n",
    "if (\"XDG_DATA_HOME\" not in os.environ) and (\"TTS_HOME\" not in os.environ):\n",
    "\tos.environ[\"XDG_DATA_HOME\"] = WORKDIR + \"\\\\temp\"\n",
    "\tos.environ[\"TTS_HOME\"] = WORKDIR + \"\\\\temp\"\n",
    "\n",
    "# Set location for input files\n",
    "REF_VOICES = WORKDIR + \"\\\\voices\"\n",
    "INPUT_TEXTS = WORKDIR + \"\\\\texts\"\n",
    "\n",
    "# Set location for output files\n",
    "OUTPUT_CHUNKS = WORKDIR + \"\\\\chunks\"\n",
    "OUTPUT_AUDIOS = WORKDIR + \"\\\\audios\"\n",
    "\n",
    "# Pick the file for now\n",
    "# Should loop through all the file later\n",
    "# VOICE_NAME = \"HorTuckLoon_Talk\"\n",
    "# VOICE_NAME = \"HorTuckLoon_GP\"\n",
    "\n",
    "FILE_NAME = \"preface\"\n",
    "# FILE_NAME = \"thought_out_thought\"\n",
    "# FILE_NAME = \"way_of_thought\"\n",
    "# FILE_NAME = \"weight_of_thought\"\n",
    "\n",
    "\n",
    "# Output tuning\n",
    "SPEED = 0.9                 # 1.0 = normal, 1.1 = 10% faster, 0.9 = 10% slower\n",
    "MP3_BITRATE = \"192k\"        # e.g. \"96k\", \"128k\", \"192k\", \"256k\"\n",
    "\n",
    "NORMALIZE_LOUDNESS = True   # loudnorm\n",
    "LOUDNORM_I = -16            # Integrated loudness target (LUFS). Common: -16 (podcast/audiobook)\n",
    "LOUDNORM_TP = -1.5          # True peak (dBTP)\n",
    "LOUDNORM_LRA = 11           # Loudness range\n",
    "\n",
    "TRIM_SILENCE = False         # silenceremove\n",
    "SILENCE_THRESHOLD_DB = -45  # threshold in dB for silence detection\n",
    "SILENCE_MIN_SEC = 0.20      # min silence duration to trim at start/end (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8764292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise text file\n",
    "# Could write another one for different purposes\n",
    "def normalize_text(text: str) -> str:\n",
    "\t# Remove hyphenation at line breaks: \"exam-\\nple\" -> \"example\"\n",
    "\ttext = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n",
    "\t# Collapse whitespace\n",
    "\ttext = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\ttext = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\treturn text.strip()\n",
    "\n",
    "def load_paragraphs(txt_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    1) Remove line breaks inside paragraphs so each paragraph becomes one line.\n",
    "    2) Return a list of paragraphs: contents = [para1, para2, ...]\n",
    "\n",
    "    Paragraphs are separated by one or more blank lines.\n",
    "    \"\"\"\n",
    "    text = Path(txt_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    paragraphs = []\n",
    "    # Split on blank lines (handles Windows/Mac/Linux newlines)\n",
    "    # However can not group into one line\n",
    "    # for block in text.split('\\n\\n'):\n",
    "    #     paragraphs.append(block)\n",
    "\n",
    "    # A robust approach: split by blank lines using regex\n",
    "    import re\n",
    "    blocks = re.split(r\"(?:\\r?\\n){2,}\", text.strip())\n",
    "\n",
    "    for b in blocks:\n",
    "        # Collapse internal newlines/whitespace into single spaces\n",
    "        one_line = re.sub(r\"\\s*\\r?\\n\\s*\", \" \", b.strip())\n",
    "        # Collapse multiple spaces/tabs\n",
    "        one_line = re.sub(r\"[ \\t]{2,}\", \" \", one_line)\n",
    "        if one_line:\n",
    "            paragraphs.append(one_line)\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690d0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare method for wav conversion as wav is lossless, no compression, better data for training\n",
    "def mp3_to_wav(mp3_path, wav_path):\n",
    "    \"\"\"Convert MP3 -> WAV using pydub (requires ffmpeg).\"\"\"\n",
    "    audio = AudioSegment.from_file(mp3_path, format=\"mp3\")\n",
    "    audio = audio.set_channels(1)  # mono often works better for speaker conditioning\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93808cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio processing functions\n",
    "def _atempo_chain(speed: float) -> str:\n",
    "    \"\"\"Build chained atempo filters to support speeds outside 0.5..2.0.\"\"\"\n",
    "    if speed <= 0:\n",
    "        raise ValueError(\"SPEED must be > 0\")\n",
    "\n",
    "    parts = []\n",
    "    x = float(speed)\n",
    "\n",
    "    while x > 2.0:\n",
    "        parts.append(\"atempo=2.0\")\n",
    "        x /= 2.0\n",
    "    while x < 0.5:\n",
    "        parts.append(\"atempo=0.5\")\n",
    "        x /= 0.5\n",
    "\n",
    "    parts.append(f\"atempo={x:.6f}\".rstrip(\"0\").rstrip(\".\"))\n",
    "    return \",\".join(parts)\n",
    "\n",
    "def _silenceremove_filter(threshold_db: float, min_sec: float) -> str:\n",
    "    \"\"\"Trim leading+trailing silence using silenceremove.\"\"\"\n",
    "    thr = f\"{threshold_db}dB\"\n",
    "    d = max(0.0, float(min_sec))\n",
    "    return (\n",
    "        f\"silenceremove=\"\n",
    "        f\"start_periods=1:start_duration={d}:start_threshold={thr}:\"\n",
    "        f\"stop_periods=1:stop_duration={d}:stop_threshold={thr}\"\n",
    "    )\n",
    "\n",
    "def _loudnorm_filter(I: float, TP: float, LRA: float) -> str:\n",
    "    \"\"\"EBU R128 loudness normalization (single-pass).\"\"\"\n",
    "    return f\"loudnorm=I={I}:TP={TP}:LRA={LRA}\"\n",
    "\n",
    "def export_with_ffmpeg_filters(\n",
    "    audio: AudioSegment,\n",
    "    out_path: Path,\n",
    "    speed: float = 1.0,\n",
    "    mp3_bitrate: str = \"192k\",\n",
    "    trim_silence: bool = True,\n",
    "    silence_threshold_db: float = -45,\n",
    "    silence_min_sec: float = 0.20,\n",
    "    normalize_loudness: bool = True,\n",
    "    loudnorm_I: float = -16,\n",
    "    loudnorm_TP: float = -1.5,\n",
    "    loudnorm_LRA: float = 11,\n",
    "):\n",
    "    \"\"\"Export AudioSegment applying FFmpeg filters: silenceremove, atempo, loudnorm, and bitrate.\"\"\"\n",
    "    if shutil.which(\"ffmpeg\") is None:\n",
    "        raise RuntimeError(\"ffmpeg not found on PATH. Install ffmpeg to use MP3 I/O and audio filtering.\")\n",
    "\n",
    "    out_path = Path(out_path)\n",
    "    out_ext = out_path.suffix.lower()\n",
    "\n",
    "    # Write a temp WAV from pydub, then let FFmpeg filter+encode.\n",
    "    tmp_wav = out_path.with_suffix(\".tmp_export.wav\")\n",
    "    audio.export(tmp_wav, format=\"wav\")\n",
    "\n",
    "    filters = []\n",
    "    if trim_silence:\n",
    "        filters.append(_silenceremove_filter(silence_threshold_db, silence_min_sec))\n",
    "    if abs(speed - 1.0) > 1e-6:\n",
    "        filters.append(_atempo_chain(speed))\n",
    "    if normalize_loudness:\n",
    "        filters.append(_loudnorm_filter(loudnorm_I, loudnorm_TP, loudnorm_LRA))\n",
    "\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", str(tmp_wav)]\n",
    "\n",
    "    if filters:\n",
    "        cmd += [\"-filter:a\", \",\".join(filters)]\n",
    "\n",
    "    if out_ext == \".mp3\":\n",
    "        cmd += [\"-b:a\", mp3_bitrate, str(out_path)]\n",
    "    elif out_ext == \".wav\":\n",
    "        cmd += [\"-c:a\", \"pcm_s16le\", str(out_path)]\n",
    "    else:\n",
    "        raise ValueError(\"OUT_AUDIO must end with .wav or .mp3\")\n",
    "\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    try:\n",
    "        tmp_wav.unlink()\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea1061",
   "metadata": {},
   "source": [
    "## 2. Extract text from provided text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old method to extract text files\n",
    "\n",
    "# file_path = f\"{INPUT_TEXTS}\\\\{FILE_NAME}.txt\"\n",
    "# contents = []\n",
    "# with open(file_path, 'r') as file:\n",
    "# \tfor line in file:\n",
    "# \t\tif line.strip():\n",
    "# \t\t\tcontents.append(normalize_text(line))\n",
    "\n",
    "# i=1\n",
    "# for para in contents:\n",
    "# \tprint(f\"{i}. {para}\")\n",
    "# \ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b7ec917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Chapter 3: THE WEIGHT OF THOUGHTS (Originally posted on Facebook, 08 September 2025)\n",
      "2. Thoughts are not as innocent as we often believe, nor as short-lived as they seem.\n",
      "3. They are not mere passing comments; on the contrary, they can breed more contempt than we realize.\n",
      "4. Thought gives rise to a personalized view, shaping the way we perceive and interpret the world around us. Often, we remain unaware of our thoughts, leaving them to run their course without realizing their effect on the mind. Why is it that thoughts are so easily taken for granted, their consequences rarely considered? There are several reasons for this oversight.\n",
      "5. Whenever we think, we treat our thoughts as meaningful, real, and true. Yet we seldom examine their deeper implications on the very mind that experiences them. This becomes even more critical if we do not understand that there are such things as wholesome and unwholesome deeds—each carrying its own further repercussions into the future.\n",
      "6. WHY THOUGHTS ESCAPE NOTICE\n",
      "7. First, when a thought arises, we rarely notice it as a thought. It feels real rather than constructed. The narrative it produces is accepted as reality, never recognized as a narrative.\n",
      "8. Second, thought is almost always projected outward. Ideas never truly leave their source; they remain mental fabrications. If we recognized this, we might pause. But the nature of thought is to appear outwardly directed, giving the impression it originates elsewhere, as if “out there” rather than within us.\n",
      "9. A classic example is judgment. When we unknowingly judge another, it feels as if the judgment is about them. We rarely notice that it arises from here and is projected there. The judgment is never truly about the other; it is a reflection of the mind projecting itself.\n",
      "10. Third, our lives have long given thoughts a sense of reality. We were never taught that thoughts are constructs rather than absolute truth. Even our education rarely addresses the workings of the mind. We have been under the spell of thought so long that everything we think feels uncensored—and often manifests in speech and action without reflection.\n",
      "11. THE STRUGGLE TO NOTICE FABRICATION\n",
      "12. Believing that thought is real becomes the default for everyone—until the right knowledge appears at one’s doorstep. Even then, at first, it is difficult to accept that thoughts are fabrications, because they have always felt real. Even with the wish to understand, the mind’s delusion prevents immediate recognition of the true nature of thoughts.\n",
      "13. Thus, one must embark on a meditative journey: to cultivate interest in thought and its consequences. Equally important is learning the skill of detachment—observing thoughts as objects rather than possessions. Only then can a thought be seen clearly, left alone without being claimed as “mine.” If it is still considered personal, it remains difficult to recognize it simply as thought.\n",
      "14. THE SEEMING REALITY OF THOUGHT\n",
      "15. Each thought, once generated, shapes our experience. No thought intended is ever entirely unreal; even anger or delusion feels justified in its own way. Only wisdom can see through the seeming reality of thought, recognizing it as a construct rather than lasting truth. Awareness is the key—through it, we may change the direction of our mind, preventing thought from unconsciously shaping our lives in ways we would not choose.\n",
      "16. Another chief reason we do not take the patterns of our thoughts seriously is that we lack realizational wisdom—wisdom that reveals that a thought never truly leaves the mind. Every thought leaves a legacy, shaping what we will become in the future, whether mentally or even materially.\n",
      "17. If we had the ability to observe the consequences of each thought in the near future, we would instantly become mindful without effort. Such understanding shows that every thought matters, influencing the course of what is to come. The stronger the thought, the greater its potential impact. Weaker thoughts may lie dormant or fade, unless repeated—because even small, habitual thoughts can accumulate into a powerful force over time.\n",
      "18. THE ERROR OF IDENTIFICATION\n",
      "19. Lastly, until we come to recognize that thoughts are merely thoughts, our ordinary experience treats them as if they are not thoughts at all. They feel like “me,” not “mine.” We take thought to be ourselves, inseparable from who we are. Because of this identification, there seems no reason to see a thought as a separate state. This is a serious error, and it is a chief reason why we fail to notice thoughts as thoughts.\n",
      "20. LIBERATION THROUGH WISDOM\n",
      "21. When wisdom arises in the mind, the ability to see thoughts as distinct—external, yet arising within the mind—becomes profoundly liberating. They no longer cling to us as if they are an intrinsic part of who we are. In this awareness, one immediately recognizes that attachment is the cause of this non-separation of thought and self, and also of the perception of reality as fixed. Attachment is the culprit. As long as this function remains unresolved, it will inevitably cause suffering, leading us to see things incorrectly.\n",
      "22. Ultimately, whatever the reasons may be, the true culprit is ignorance of the nature of thought—and our attachment to it. To chart the way of thoughts wisely, we need a clear understanding of what is wholesome and what is unwholesome. It is about understanding the gravity and consequences of each thought, and how they are fabricated.\n"
     ]
    }
   ],
   "source": [
    "# Parse text file into `contents` object\n",
    "file_path = f\"{INPUT_TEXTS}\\\\{FILE_NAME}.txt\"\n",
    "contents = load_paragraphs(file_path)\n",
    "\n",
    "# Print out chunk texts for monitoring text quality before TTS\n",
    "i=1\n",
    "for para in contents:\n",
    "\tprint(f\"{i}. {para}\")\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d03731",
   "metadata": {},
   "source": [
    "## 2. Load xTTSv2 model into `tts` intance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22727b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TTS model, download xtts_v2 model to local\n",
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
    "\n",
    "# After download, run the model locally\n",
    "tts = TTS(model_path=\"models\\\\tts_models--multilingual--multi-dataset--xtts_v2\\\\model.pth\",\n",
    "\t\t  config_path=\"models\\\\tts_models--multilingual--multi-dataset--xtts_v2\\\\config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66afb3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tts_paths(tts: \"TTS\") -> dict:\n",
    "#     '''\n",
    "# \tSupport function to check location of model\n",
    "#     '''\n",
    "#     if tts.synthesizer is None:\n",
    "#         raise RuntimeError(\"No TTS synthesizer loaded (did you load a VC model instead?)\")\n",
    "\n",
    "#     ckpt = Path(tts.synthesizer.tts_checkpoint)\n",
    "#     cfg  = Path(tts.synthesizer.tts_config_path)\n",
    "\n",
    "#     # If checkpoint is a file, model_root is its parent; if it's a dir, it is the dir itself.\n",
    "#     model_root = ckpt if ckpt.is_dir() else ckpt.parent\n",
    "\n",
    "#     return {\n",
    "#         \"model_name\": tts.model_name,\n",
    "#         \"checkpoint_path\": str(ckpt),\n",
    "#         \"config_path\": str(cfg),\n",
    "#         \"model_root\": str(model_root),\n",
    "#         \"voice_dir\": str(tts.synthesizer.voice_dir),\n",
    "#         \"download_base_default\": str(tts.manager.output_prefix),  # base folder for non-HF models\n",
    "#     }\n",
    "\n",
    "# paths = get_tts_paths(tts)\n",
    "# paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47fa5d",
   "metadata": {},
   "source": [
    "## 3. Cache reference voices for easy reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885d0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the voice from reference audios - Dont have to run everytime\n",
    "# reference_files = []\n",
    "# for file in Path(REF_VOICES).iterdir():\n",
    "# \treference_files.append(f\"{REF_VOICES}\\\\{file.name}\")\n",
    "# output_voice = f\"{OUTPUT_CHUNKS}\\\\test_voice.wav\"\n",
    "\n",
    "# if reference_files:\n",
    "# \ttts.tts_to_file(\n",
    "# \t\ttext=\"This is the audio edition of the book - Thought - Our Sole Universe\",\n",
    "# \t\tspeaker_wav=reference_files,\n",
    "# \t\t# Assign `speakerID` to reuse later\n",
    "# \t\tspeaker=\"HorTuckLoon\",\n",
    "# \t\tlanguage=\"en\",\n",
    "# \t\tfile_path=output_voice\n",
    "# \t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65234b94",
   "metadata": {},
   "source": [
    "## 4. Generate audio chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99deae6",
   "metadata": {},
   "source": [
    "Create a temporary folder like `thought_out_thought/` with many chunk output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either reset or leave untouch the chunk_files object on a new run\n",
    "chunk_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for paragraph in contents:\n",
    "\t# names of temporary chunks files\n",
    "\tfile_name = f\"{OUTPUT_CHUNKS}\\\\{FILE_NAME}\\\\output{i}.wav\"\n",
    "\tif not os.path.exists(file_name):\n",
    "\t\ttts.tts_to_file(\n",
    "\t\t\ttext=paragraph,\n",
    "\t\t\tspeaker=\"HorTuckLoon\",\n",
    "\t\t\tlanguage=\"en\",\n",
    "\t\t\tfile_path=file_name\n",
    "\t\t)\n",
    "\t\tchunk_files.append(file_name)\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378be76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# Sort the list to make sure the chunks is in the right orders\n",
    "def natural_sort_key(s):\n",
    "    # Split the filename into parts of numbers and non-numbers\n",
    "    return [int(p) if p.isdigit() else p.lower() for p in re.findall(r'\\d+|\\D+', s)]\n",
    "\n",
    "chunk_files.sort(key=natural_sort_key)\n",
    "print(chunk_files.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713a226",
   "metadata": {},
   "source": [
    "## 5. Concatenate into a single audio file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: ffmpeg -y -i d:\\Dhamma\\AudioFileTool\\audios\\weight_of_thought.tmp_export.wav -filter:a atempo=0.9,loudnorm=I=-16:TP=-1.5:LRA=11 -b:a 192k d:\\Dhamma\\AudioFileTool\\audios\\weight_of_thought.mp3\n",
      "Generated d:\\Dhamma\\AudioFileTool\\audios\\weight_of_thought.mp3!\n"
     ]
    }
   ],
   "source": [
    "# Concatenate chunk WAVs into one AudioSegment\n",
    "combined = AudioSegment.empty()\n",
    "audio_file = f\"{OUTPUT_AUDIOS}\\\\{FILE_NAME}.mp3\"\n",
    "\n",
    "for file in chunk_files:\n",
    "    seg = AudioSegment.from_wav(file)\n",
    "    combined += seg\n",
    "\n",
    "try:\n",
    "\texport_with_ffmpeg_filters(\n",
    "\t\taudio=combined,\n",
    "\t\tout_path=audio_file,\n",
    "\t\tspeed=SPEED,\n",
    "\t\tmp3_bitrate=MP3_BITRATE,\n",
    "\t\ttrim_silence=TRIM_SILENCE,\n",
    "\t\tsilence_threshold_db=SILENCE_THRESHOLD_DB,\n",
    "\t\tsilence_min_sec=SILENCE_MIN_SEC,\n",
    "\t\tnormalize_loudness=NORMALIZE_LOUDNESS,\n",
    "\t\tloudnorm_I=LOUDNORM_I,\n",
    "\t\tloudnorm_TP=LOUDNORM_TP,\n",
    "\t\tloudnorm_LRA=LOUDNORM_LRA,\n",
    "\t)\n",
    "\tprint(f\"Generated {audio_file}!\")\n",
    "except:\n",
    "    print(f\"Error couldn't combine chunk files!\")\n",
    "# try:\n",
    "#     combined.export(audio_file, format=\"mp3\", bitrate=\"192k\")\n",
    "# except:\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
