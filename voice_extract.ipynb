{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6debf667",
   "metadata": {},
   "source": [
    "##### Scripts for:\n",
    "\n",
    "[x]. Analyzing an audio file to extract/clone a voice,\n",
    "\n",
    "[]. Generating new audio files from text in that cloned voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d03731",
   "metadata": {},
   "source": [
    "##### 1. Coqui TTS has voice cloning support where you can provide reference audio files and generate speech with that voice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement python (from versions: none)\n",
      "ERROR: No matching distribution found for python\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m venv audiogen\n",
    "!audiogen/Scripts/activate\n",
    "!pip install ipykernel, coqui-tts\n",
    "!uv pip install torch torchaudio torchcodec --torch-backend=auto\n",
    "!cd coqui-ai-TTS\n",
    "!uv pip install -e .[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b169af3",
   "metadata": {},
   "source": [
    "```\n",
    "python -m venv audiogen\n",
    "audiogen/Scripts/activate\n",
    "pip install ipykernel, coqui-tts\n",
    "uv pip install torch torchaudio torchcodec --torch-backend=auto\n",
    "cd coqui-ai-TTS\n",
    "uv pip install -e .[notebooks]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33d57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from TTS.api import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35508db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COQUI_TOS_AGREED\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59d4d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tts_models/multilingual/multi-dataset/xtts_v2', 'tts_models/multilingual/multi-dataset/xtts_v1.1', 'tts_models/multilingual/multi-dataset/your_tts', 'tts_models/multilingual/multi-dataset/bark', 'tts_models/bg/cv/vits', 'tts_models/cs/cv/vits', 'tts_models/da/cv/vits', 'tts_models/et/cv/vits', 'tts_models/ga/cv/vits', 'tts_models/en/ek1/tacotron2', 'tts_models/en/ljspeech/tacotron2-DDC', 'tts_models/en/ljspeech/tacotron2-DDC_ph', 'tts_models/en/ljspeech/glow-tts', 'tts_models/en/ljspeech/speedy-speech', 'tts_models/en/ljspeech/tacotron2-DCA', 'tts_models/en/ljspeech/vits', 'tts_models/en/ljspeech/vits--neon', 'tts_models/en/ljspeech/fast_pitch', 'tts_models/en/ljspeech/overflow', 'tts_models/en/ljspeech/neural_hmm', 'tts_models/en/vctk/vits', 'tts_models/en/vctk/fast_pitch', 'tts_models/en/sam/tacotron2-DCA', 'tts_models/en/blizzard2013/capacitron-t2-c50', 'tts_models/en/blizzard2013/capacitron-t2-c150_v2', 'tts_models/en/multi-dataset/tortoise-v2', 'tts_models/en/jenny/jenny', 'tts_models/es/mai/tacotron2-DDC', 'tts_models/es/css10/vits', 'tts_models/fr/mai/tacotron2-DDC', 'tts_models/fr/css10/vits', 'tts_models/uk/mai/glow-tts', 'tts_models/uk/mai/vits', 'tts_models/zh-CN/baker/tacotron2-DDC-GST', 'tts_models/nl/rdh/tacotron2-DDC', 'tts_models/nl/css10/vits', 'tts_models/de/thorsten/tacotron2-DCA', 'tts_models/de/thorsten/vits', 'tts_models/de/thorsten/tacotron2-DDC', 'tts_models/de/css10/vits-neon', 'tts_models/ja/kokoro/tacotron2-DDC', 'tts_models/tr/common-voice/glow-tts', 'tts_models/it/mai_female/glow-tts', 'tts_models/it/mai_female/vits', 'tts_models/it/mai_male/glow-tts', 'tts_models/it/mai_male/vits', 'tts_models/ewe/openbible/vits', 'tts_models/hau/openbible/vits', 'tts_models/lin/openbible/vits', 'tts_models/tw_akuapem/openbible/vits', 'tts_models/tw_asante/openbible/vits', 'tts_models/yor/openbible/vits', 'tts_models/hu/css10/vits', 'tts_models/el/cv/vits', 'tts_models/fi/css10/vits', 'tts_models/hr/cv/vits', 'tts_models/lt/cv/vits', 'tts_models/lv/cv/vits', 'tts_models/mt/cv/vits', 'tts_models/pl/mai_female/vits', 'tts_models/pt/cv/vits', 'tts_models/ro/cv/vits', 'tts_models/sk/cv/vits', 'tts_models/sl/cv/vits', 'tts_models/sv/cv/vits', 'tts_models/ca/custom/vits', 'tts_models/fa/custom/glow-tts', 'tts_models/fa/custom/vits-female', 'tts_models/bn/custom/vits-male', 'tts_models/bn/custom/vits-female', 'tts_models/be/common-voice/glow-tts', 'vocoder_models/universal/libri-tts/wavegrad', 'vocoder_models/universal/libri-tts/fullband-melgan', 'vocoder_models/en/ek1/wavegrad', 'vocoder_models/en/librispeech100/wavlm-hifigan', 'vocoder_models/en/librispeech100/wavlm-hifigan_prematched', 'vocoder_models/en/ljspeech/multiband-melgan', 'vocoder_models/en/ljspeech/hifigan_v2', 'vocoder_models/en/ljspeech/univnet', 'vocoder_models/en/blizzard2013/hifigan_v2', 'vocoder_models/en/vctk/hifigan_v2', 'vocoder_models/en/sam/hifigan_v2', 'vocoder_models/nl/mai/parallel-wavegan', 'vocoder_models/de/thorsten/wavegrad', 'vocoder_models/de/thorsten/fullband-melgan', 'vocoder_models/de/thorsten/hifigan_v1', 'vocoder_models/ja/kokoro/hifigan_v1', 'vocoder_models/uk/mai/multiband-melgan', 'vocoder_models/tr/common-voice/hifigan', 'vocoder_models/be/common-voice/hifigan', 'voice_conversion_models/multilingual/vctk/freevc24', 'voice_conversion_models/multilingual/multi-dataset/knnvc', 'voice_conversion_models/multilingual/multi-dataset/openvoice_v1', 'voice_conversion_models/multilingual/multi-dataset/openvoice_v2']\n"
     ]
    }
   ],
   "source": [
    "# Get device if NVDIA GPU is present. AMD GPU is not supported or very limit.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(TTS().list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22727b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25588f3eafc4e79a6139c8ebf9686ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366cf5a10c7346729c87245b7ee8f700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6 and 7.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 7: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# clone from reference audio\u001b[39;00m\n\u001b[32m      5\u001b[39m reference_files = [\u001b[33m\"\u001b[39m\u001b[33mreference_voice.mp3\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThis is a test of my cloned voice!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\api.py:368\u001b[39m, in \u001b[36mTTS.tts_to_file\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, pipe_out, file_path, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert text to speech.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m    341\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m \u001b[33;03m        Additional arguments for the model.\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[38;5;28mself\u001b[39m.synthesizer.save_wav(wav=wav, path=file_path, pipe_out=pipe_out)\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\api.py:317\u001b[39m, in \u001b[36mTTS.tts\u001b[39m\u001b[34m(self, text, speaker, language, speaker_wav, emotion, split_sentences, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m    316\u001b[39m \u001b[38;5;28mself\u001b[39m._check_arguments(speaker=speaker, language=language, speaker_wav=speaker_wav, emotion=emotion, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m wav = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msynthesizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\utils\\synthesizer.py:405\u001b[39m, in \u001b[36mSynthesizer.tts\u001b[39m\u001b[34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, source_wav, source_speaker_name, split_sentences, return_dict, **kwargs)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source_wav:  \u001b[38;5;66;03m# not voice conversion\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sens:\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtts_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvoice_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvoice_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m            \u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_griffin_lim\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_gl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m use_gl:\n\u001b[32m    415\u001b[39m             waveform = outputs[\u001b[33m\"\u001b[39m\u001b[33mwav\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\tts\\models\\xtts.py:388\u001b[39m, in \u001b[36mXtts.synthesize\u001b[39m\u001b[34m(self, text, config, speaker, speaker_wav, voice_dir, language, **kwargs)\u001b[39m\n\u001b[32m    386\u001b[39m     gpt_cond_latent, speaker_embedding = \u001b[38;5;28mself\u001b[39m.speaker_manager.speakers[speaker].values()\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     voice = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclone_voice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mvoice_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m     gpt_cond_latent = voice[\u001b[33m\"\u001b[39m\u001b[33mgpt_conditioning_latents\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    390\u001b[39m     speaker_embedding = voice[\u001b[33m\"\u001b[39m\u001b[33mspeaker_embedding\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\utils\\voices.py:101\u001b[39m, in \u001b[36mCloningMixin.clone_voice\u001b[39m\u001b[34m(self, speaker_wav, speaker_id, voice_dir, **generate_kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_voice_file(speaker_id, voice_dir)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m voice, model_metadata = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clone_voice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mGenerated voice from reference audio\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m speaker_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m voice_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\tts\\models\\xtts.py:268\u001b[39m, in \u001b[36mXtts._clone_voice\u001b[39m\u001b[34m(self, speaker_wav, **generate_kwargs)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_clone_voice\u001b[39m(\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m, speaker_wav: \u001b[38;5;28mstr\u001b[39m | os.PathLike[Any] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m | os.PathLike[Any]], **generate_kwargs: Any\n\u001b[32m    267\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     gpt_conditioning_latents, speaker_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_conditioning_latents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeaker_wav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     voice = {\u001b[33m\"\u001b[39m\u001b[33mgpt_conditioning_latents\u001b[39m\u001b[33m\"\u001b[39m: gpt_conditioning_latents, \u001b[33m\"\u001b[39m\u001b[33mspeaker_embedding\u001b[39m\u001b[33m\"\u001b[39m: speaker_embedding}\n\u001b[32m    273\u001b[39m     metadata = {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\tts\\models\\xtts.py:308\u001b[39m, in \u001b[36mXtts.get_conditioning_latents\u001b[39m\u001b[34m(self, audio_path, max_ref_length, gpt_cond_len, gpt_cond_chunk_len, librosa_trim_db, sound_norm_refs, load_sr)\u001b[39m\n\u001b[32m    306\u001b[39m speaker_embedding = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m audio_paths:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     audio = \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_sr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     audio = audio[:, : load_sr * max_ref_length].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sound_norm_refs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dhamma\\AudioFileTool\\coqui-ai-TTS\\TTS\\tts\\models\\xtts.py:87\u001b[39m, in \u001b[36mload_audio\u001b[39m\u001b[34m(audiopath, sampling_rate)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_audio\u001b[39m(audiopath, sampling_rate):\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# better load setting following: https://github.com/faroit/python_audio_loading_benchmark\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# torchaudio should chose proper backend to load audio depending on platform\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     audio, lsr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudiopath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# stereo to mono if needed\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m audio.size(\u001b[32m0\u001b[39m) != \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchaudio\\__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchaudio\\_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Import torchcodec here to provide clear error if not available\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\__init__.py:10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Note: usort wants to put Frame and FrameBatch after decoders and samplers,\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# but that results in circular import.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\decoders\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_video_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VideoDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\_core\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AudioStreamMetadata,\n\u001b[32m     10\u001b[39m     ContainerMetadata,\n\u001b[32m     11\u001b[39m     get_container_metadata,\n\u001b[32m     12\u001b[39m     get_container_metadata_from_header,\n\u001b[32m     13\u001b[39m     VideoStreamMetadata,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     _add_video_stream,\n\u001b[32m     17\u001b[39m     _get_key_frame_indices,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     seek_to_pts,\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\_core\\_metadata.py:16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _get_container_json_metadata,\n\u001b[32m     18\u001b[39m     _get_stream_json_metadata,\n\u001b[32m     19\u001b[39m     create_from_file,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     23\u001b[39m SPACES = \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStreamMetadata\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\_core\\ops.py:84\u001b[39m\n\u001b[32m     64\u001b[39m     traceback = (\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     67\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33m          1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n\u001b[32m     89\u001b[39m create_from_file = torch._dynamo.disallow_in_graph(\n\u001b[32m     90\u001b[39m     torch.ops.torchcodec_ns.create_from_file.default\n\u001b[32m     91\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\_core\\ops.py:69\u001b[39m, in \u001b[36mload_torchcodec_shared_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     62\u001b[39m         exceptions.append((ffmpeg_major_version, e))\n\u001b[32m     64\u001b[39m traceback = (\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     67\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     70\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m         versions 4, 5, 6 and 7.\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is not compatible with\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[33m         table:\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[33m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6 and 7.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 7: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: D:\\Dhamma\\AudioFileTool\\audiogen\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback]."
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize TTS model\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\")\n",
    "\n",
    "# clone from reference audio\n",
    "reference_files = [\"reference_voice.mp3\"]\n",
    "tts.tts_to_file(\n",
    "    text=\"This is a test of my cloned voice!\",\n",
    "    speaker_wav=reference_files,\n",
    "    language=\"en\",\n",
    "    file_path=\"output.wav\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
